# Teste TÃ©cnico - Monitoramento de Operadoras ANS

SoluÃ§Ã£o fullstack para automaÃ§Ã£o de coleta (ETL), armazenamento e visualizaÃ§Ã£o de dados contÃ¡beis e cadastrais de operadoras de planos de saÃºde, utilizando dados abertos da ANS.

---

## ğŸ› ï¸ Tecnologias utilizadas e DecisÃµes tÃ©cnicas

### 1. Linguagem: Python ğŸ

**Escolha:** Para iniciar o projeto, foi pedido para escolher uma entre duas linguagens de programaÃ§Ã£o: Python e Java.
**Justificativa:** Devido a sua flexibilidade, agilidade e facilidade, nÃ£o sÃ³ pela escrita do cÃ³digo, mas tambÃ©m pela integraÃ§Ã£o com o FRONTEND e sua proficiÃªncia em lidar com arquivos (leitura, download, criaÃ§Ã£o), especialmente arquivos com extensÃ£o csv, por haver bibliotecas como o pandas e sqlAlchemy que facilitem essa leitura e tambÃ©m integraÃ§Ã£o direta com banco de dados.

### 2. Banco de dados: PostgreSQL ğŸ˜

**Escolha:** O segundo passo antes de iniciar o projeto foi escolher qual software SQL seria usado para trabalhar com banco de dados.
**Justificativa:** Optei pelo PostgreSQL devido Ã  sua robustez, conformidade com ACID e familiaridade prÃ©via. Ele lida excelentemente com integridade referencial, o que Ã© crucial para relacionar as tabelas de `Operadoras` e `Despesas`. Ademais, com prÃ©vias experiÃªncias com o PostgreSQL e seu software jÃ¡ instalado na mÃ¡quina local, isso deixou a escolha ainda mais Ã³bvia.

### 3. Arquitetura do cÃ³digo

**Escolha:** CÃ³digos separados por funcionalidade e Backend separado do Frontend.
**Justificativa:** No Backend foram separados de tal forma que cada arquivo fosse uma etapa do processo de desenvolvimento, nomeados como etapa1, etapa2 e etapa3. Ademais, cada arquivo possui arquitetura de cÃ³digo limpo: funcionalidades Ãºnicas por funÃ§Ã£o. No frontend os componentes foram dividos entre componente principal e pÃ¡gina de detalhes, garantindo melhor organizaÃ§Ã£o do cÃ³digo e facilidade com rotas. Por fim, a separaÃ§Ã£o entre Backend e Frontend garante que o sistema seja escalÃ¡vel e que o processamento de dados (pesado) nÃ£o impacte a experiÃªncia do usuÃ¡rio na interface.

---

## ğŸ“‹ PrÃ©-requisitos

Para executar este projeto, serÃ¡ necessÃ¡rio:

- **Python 3.10+**
- **Node.js** (v16 ou superior) & **npm**
- **PostgreSQL** (Instalado e rodando na porta 5432)

---

## ğŸš€ Guia de instalaÃ§Ã£o e configuraÃ§Ã£o

Siga os passos abaixo na ordem apresentada para configurar o ambiente.

### 1. ConfiguraÃ§Ã£o do Backend

1.  Acesse a pasta do backend:

    ```bash
    cd backend
    ```

2.  Crie um ambiente virtual:

    ```bash
    python -m venv venv
    # Windows:
    venv\Scripts\activate
    # Linux/Mac:
    source venv/bin/activate
    ```

3.  Instale as dependÃªncias:

    ```bash
    pip install -r requirements.txt
    ```

4.  **VariÃ¡veis de ambiente (.env):**
    - Renomeie o arquivo `.env.example` para `.env`.
    - Edite o arquivo `.env` e coloque o usuÃ¡rio e senha do **seu** banco de dados PostgreSQL local e o nome do banco de dados criado na variÃ¡vel `DATABASE_URL`.

### 2. ConfiguraÃ§Ã£o do Banco de dados

1.  Abra seu gerenciador de banco (pgAdmin, DBeaver ou terminal).
2.  Crie um banco de dados vazio chamado: **`datas_info`**.
    - _O script Python criarÃ¡ as tabelas automaticamente, nÃ£o precisa criar tabelas manualmente._

### 3. ConfiguraÃ§Ã£o do Frontend

1.  Em um novo terminal, acesse a pasta do frontend:
    ```bash
    cd frontend
    ```
2.  Instale as dependÃªncias e configure o ambiente:
    ```bash
    npm install
    ```
    Renomeie o arquivo `.env.example` para `.env`.

---

## â–¶ï¸ Como Executar os cÃ³digos

Execute os scripts Python na ordem abaixo para realizar o processo completo de ETL (Extract, Transform, Load).
**âš ï¸ ATENÃ‡ÃƒO:** Antes de executar os cÃ³digos, verifique no terminal se o cÃ³digo estÃ¡ sendo executado dentro da pasta **backend** e se o **venv** estÃ¡ ativado (passos 1 a 4).

### Passo 1: AutomaÃ§Ã£o de download

Este script conecta no FTP da ANS e baixa os arquivos ZIP (DemonstraÃ§Ãµes ContÃ¡beis) e CSV (RelatÃ³rio_cadop) automaticamente.

```bash
python script_download.py
```

##### Resultado: Cria a pasta ./assets com os arquivos brutos (zip e csv).

### Passo 2: ExtraÃ§Ã£o e tratamento de dados

Extrai os ZIPs, filtra as despesas de "Eventos/Sinistros", limpa os dados e consolida em um Ãºnico CSV (consolidado_despesas) e joga esses arquivos numa pasta ./files que serÃ¡ criada.

```bash
python etapa1_process_file.py
```

##### Resultado: Cria a pasta ./files com os arquivos unzipados e o arquivo consolidado_despesas.csv e o zip dele.

### Passo 3: ValidaÃ§Ã£o de dados e merge de CSVs

LÃª o arquivo RelatÃ³rio_cadop.csv, valida dados: cnpj, razÃ£o social vazia e nÃºmeros negativos. Cria um arquivo despesas_agregadas com informaÃ§Ãµes de cada operadora total de despesas + o desafio adicional: mÃ©dia por trimestre e desvio padrÃ£o. Por fim faz o merge entre o arquivo consolidado_despesas.csv e o Relatorio_cadop.csv.

```bash
python etapa2_validatingData.py
```

##### Resultado: Cria o arquivo despesas_agregadas.csv com as informaÃ§Ãµes de valores de cada operadora e o arquivo relatorio_final.csv com o merge entre consolidado_despesas.csv + Relatorio_cadop.csv, alÃ©m de o zip do despesas_agregadas.csv chamado Teste_Gustavo_Luiz.zip, como pedido.

### Passo 4: Popular Banco de dados

LÃª os arquivos processados, inicia o banco de dados PostgreSQL, cria as tabelas e popula as tabelas no PostgreSQL. Por fim, cria as 3 queries pedidas:

1. Quais as 5 operadoras com maior crescimento percentual de despesas entre o primeiro e o Ãºltimo trimestre analisado? + desafio.

2. Qual a distribuiÃ§Ã£o de despesas por UF? Liste os 5 estados com maiores despesas totais + desafio adicional.

3. Quantas operadoras tiveram despesas acima da mÃ©dia geral em pelo menos 2 dos 3 trimestres analisados? + trade-off.

```bash
python etapa3_integratingDB.py
```

##### Resultado: Tabelas populadas no banco datas_info, alÃ©m de retornar a resposta para as trÃªs queries pedidas.

### Passo 5: Iniciar a API (Backend)

```bash
uvicorn main:app --reload
```

**Acesse a documentaÃ§Ã£o automÃ¡tica (Swagger) em: http://localhost:8000/docs**

### Passo 6: Iniciar o dashboard (Frontend)

```bash
npm run dev
```

**Acesse a aplicaÃ§Ã£o em: http://localhost:5173**

---

## ğŸ“š DocumentaÃ§Ã£o da API

Conforme solicitado, foi criada uma coleÃ§Ã£o do Postman contendo todas as rotas e exemplos de respostas.

- **Arquivo:** `./postman_collection.json` (Na raÃ­z do projeto)
- **Como usar:** Importe este arquivo no seu Postman para testar as rotas prÃ©-configuradas.
- **Alternativa:** A documentaÃ§Ã£o tambÃ©m estÃ¡ disponÃ­vel via Swagger em `http://localhost:8000/docs`.

---

## âš–ï¸ Trade-offs e decisÃµes tÃ©cnicas

A seguir, segue as justificativas por cada decisÃ£o tomada nos trade-offs. Cada um enumerada de acordo com o item a que ela pertence.

##### ObservaÃ§Ã£o. Para a manipulaÃ§Ã£o dos arquivos csv eu utilizei a biblioteca Pandas. Embora fosse possÃ­vel utilizar apenas as ferramentas nativas do Python (csv module), a escolha pelo Pandas se justifica pela praticidade e eficiÃªncia (princÃ­pio KISS solicitado no teste). O Pandas oferece performance superior no processamento de grandes volumes de dados e reduz drasticamente a complexidade do cÃ³digo para normalizaÃ§Ã£o e filtragem, facilitando a manutenÃ§Ã£o futura. AlÃ©m de sua forte integridade com a ORM SQLAlchemy que possibilita de forma mais fÃ¡cil e rÃ¡pida o transporte de informaÃ§Ãµes do arquivo csv para o Banco de dados.

### Item 1: TESTE DE INTEGRAÃ‡ÃƒO COM API PÃšBLICA

#### 1 -> 1.2. Processamento de Arquivos

- Para o processamento de arquivos, a segunda opÃ§Ã£o (processar incrementalmente) foi a mais viÃ¡vel. Pois, evita carregar todos os arquivos dos dados trimestrais na memÃ³ria simultaneamente e causar um uso extremo de memÃ³ria que pode acabar crashando o servidor, dependendo da quantidade de memÃ³ria RAM que o usuÃ¡rio tem disponÃ­vel e se o nÃºmero de dados aumentar posteriormente. Logo, o script itera arquivo por arquivo, extrai apenas os dados desejados (Eventos/Sinistros) guarda o resultado em um arquivo e o restante Ã© passado em branco pelo script.
  - **Vantagem:** Leve e rÃ¡pido.
  - **Desvantagem:** A escrita cÃ³digo pode ser mais trabalhosa.

##### ObservaÃ§Ã£o: Os arquivos fornecidos estÃ£o apenas em pdf, porÃ©m o cÃ³digo jÃ¡ possui estrutura para adicionar leitores de XLSX e TXT posteriormente.

#### 1 -> 1.3. ConsolidaÃ§Ã£o e AnÃ¡lise de InconsistÃªncias

- Nesse item foi solicitado a anÃ¡lise crÃ­tica de inconsistÃªncias em CNPJs. PorÃ©m, foi notÃ³rio que o arquivo com os dados iniciais ("DemonstraÃ§Ãµes ContÃ¡beis") utiliza apenas o identificador REG_ANS (Registro ANS) e nÃ£o contÃ©m os campos CNPJ ou RazÃ£o Social. Dessa forma, mantive a estrutura solicitada preenchida com valores nulos nesta etapa e deixei a validaÃ§Ã£o de CNPJ para a etapa 2.1, onde serÃ¡ feito o enriquecimento com a o relatÃ³rio cadop. A anÃ¡lise de duplicidade foi feita com base no REG_ANS."
  - **DecisÃ£o:** Optei por estruturar o CSV com as colunas solicitadas, mas mantive o campo RegistroANS (chave primÃ¡ria original) e deixei CNPJ/RazaoSocial como espaÃ§os vazios. Esses dados serÃ£o preenchidos corretamente na etapa 2.2 atravÃ©s do enriquecimento de dados (JOIN com o relatÃ³rio cadop), garantindo a integridade da informaÃ§Ã£o sem inventar dados na etapa de extraÃ§Ã£o."

### Item 2: TESTE DE TRANSFORMAÃ‡ÃƒO E VALIDAÃ‡ÃƒO DE DADOS

#### 2 -> 2.1. ValidaÃ§Ã£o de Dados com EstratÃ©gias Diferentes

- O item solicitava a validaÃ§Ã£o de CNPJ no arquivo consolidado. Como este arquivo nÃ£o possuÃ­a a informaÃ§Ã£o originalmente, realizei primeiramente o enriquecimento dos dados (Passo 2.2) para obter os CNPJs e, posteriormente, apliquei a validaÃ§Ã£o de formato e dÃ­gitos verificadores conforme solicitado.
  Feito isso, foi possÃ­vel decidir como tratar CNPJs invÃ¡lidos: adotei uma estratÃ©gia de Auditoria (Non-destructive cleaning). Em vez de excluir registros com CNPJs invÃ¡lidos, RazÃ£o Social vazia ou valores negativos, optei por criar uma coluna de metadados chamada Status_Validacao responsÃ¡vel por **marcar** cada registro, a fim de permitir identificaÃ§Ã£o de inconsistÃªncias. Pois, em sistemas financeiros e contÃ¡beis, a exclusÃ£o silenciosa de registros problemÃ¡ticos pode gerar divergÃªncias nos balanÃ§os finais (perda de rastreabilidade do valor total).
  - **Como foi implementado:** Registros corretos recebem a flag "VÃ¡lido", enquanto registros invÃ¡lidos recebem a flag "InvÃ¡lido" com o motivo detalhado (ex: "CNPJ InvÃ¡lido", "Valor Negativo"). Para a geraÃ§Ã£o do relatÃ³rio de despesas (despesas_agregadas.csv), utilizei apenas os dados com status "VÃ¡lido" para garantir a integridade das estatÃ­sticas, mas sem excluir os inconsistentes para nÃ£o perder valores importantes.

#### 2 -> 2.2. Enriquecimento de Dados com Tratamento de Falhas

- O item 2.2 instrui realizar o join utilizando o CNPJ como chave. Contudo, devido Ã  ausÃªncia dessa informaÃ§Ã£o no arquivo primÃ¡rio (DemonstraÃ§Ãµes ContÃ¡beis), utilizei o campo RegistroANS (presente em ambas as bases) como chave (key), garantindo a integridade do cruzamento e permitindo a correta importaÃ§Ã£o dos dados de CNPJ, RazÃ£o Social, UF e Modalidade.

- Para responder o trade-off tÃ©cnico, optei pela utilizaÃ§Ã£o do **Left Join** (mantendo a base de Despesas como _Left_).
  - **Justificativa:** A prioridade do projeto Ã© a integridade contÃ¡bil. Utilizar um `Inner Join` descartaria automaticamente despesas de operadoras que nÃ£o constam no arquivo de "Operadoras Ativas" (ex: empresas canceladas, em liquidaÃ§Ã£o ou com divergÃªncia cadastral), gerando um balanÃ§o financeiro incompleto.
  - **Resultado:** Todas as despesas foram preservadas. Registros sem correspondÃªncia no cadastro tiveram os campos de identificaÃ§Ã£o (CNPJ, RazÃ£o Social) preenchidos como nulos para posterior auditoria.

- Durante o desenvolvimento do cÃ³digo, foi identificada a existÃªncia de **Registros Ã“rfÃ£os** (operadoras com lanÃ§amentos de despesas, mas ausentes no arquivo de cadastro de ativas (cadop)), o que gerava incompatibilidade ao fazer o JOIN, pois Ã© como se houvesse "operadoras fantasmas". Ã‰ possÃ­vel enxergar com mais detalhes no console ao executar o arquivo `etapa2_validatingData.py`.
  - **Tratamento:** Esses registros **nÃ£o foram excluÃ­dos**. Foram mantidos no dataset final para garantir que o valor total das despesas (soma) bata com a origem, mas receberam flag de validaÃ§Ã£o correspondente.

- Para a parte de validaÃ§Ã£o de dados proposta anteriormente nos items 1.3 e 2.1 foi implementada uma rotina de validaÃ§Ã£o (`general_validation`), gerando a coluna de metadados `Status_Validacao`, jÃ¡ citada anteriormente no item 2.1. As regras aplicadas foram:
  1. **ValidaÃ§Ã£o de CNPJ:** VerificaÃ§Ã£o de formato (14 dÃ­gitos) e cÃ¡lculo matemÃ¡tico dos DÃ­gitos Verificadores (MÃ³dulo 11).
  2. **ValidaÃ§Ã£o de Valores:** Flag em despesas com valor negativo.
  3. **Dado vazio:** VerificaÃ§Ã£o de RazÃ£o Social vazia ou nula.

#### 2 -> 2.3. AgregaÃ§Ã£o com MÃºltiplas EstratÃ©gias

- Para a ordenaÃ§Ã£o preferi escolher o mÃ©todo sort_values, realizada em memÃ³ria via Pandas antes da exportaÃ§Ã£o para CSV. Pois, dado o volume de dados processado, o custo computacional de ordenar em memÃ³ria Ã© desprezÃ­vel se comparado ao benefÃ­cio de entregar um relatÃ³rio jÃ¡ priorizado para o usuÃ¡rio final, facilitando a identificaÃ§Ã£o imediata das maiores despesas sem necessidade de pÃ³s-processamento em ferramentas como Excel. Se estivÃ©ssemos lidando com Big Data, a ordenaÃ§Ã£o seria postergada para a camada de visualizaÃ§Ã£o para economizar recursos de processamento distribuÃ­do.

### Item 3: TESTE DE BANCO DE DADOS E ANÃLISE

#### 3 -> 3.2. Crie queries DDL para estruturar as tabelas necessÃ¡rias:

- Optei pela OpÃ§Ã£o B: criar tabelas normalizadas separadas para Agregados, Operadoras e Despesas, ligadas por chave estrangeira (registro_ans). Pois, a normalizaÃ§Ã£o evita redundÃ¢ncia de dados cadastrais (ex: repetir a RazÃ£o Social milhares de vezes na tabela de despesas), economizando armazenamento e facilitando atualizaÃ§Ãµes cadastrais (basta alterar em um lugar).

- Para os tipos de dados, eu escolhi o DECIMAL, (NUMERIC no SQLAlchemy) para numÃ©ricos e DATE para datas.
  - **NumÃ©rico:** Utilizar o tipo INTEGER Ã© inviÃ¡vel nesse sentido, uma vez que estamos trabalhando com valores monetÃ¡rios, que podem vim quebrados por centavos. Entre o FLOAT e DECIMAL, o FLOAT nÃ£o Ã© o melhor, pois ele tem problemas com o ponto flutuente, podendo fazer arredondamentos equivocados, logo, o DECIMAL se sobressai.
  - **Data:** Quando as datas sÃ£o importante para um registro (como Ã© o nosso caso), o VARCHAR nÃ£o Ã© recomendado pois ele armazena datas como string, e isso impossibilita operaÃ§Ãµes diretas com as datas, pois eu teria que fazer um processo para converter o tipo texto para DATE posteriormente. O timestamp armazena data e hora, nos registros, as informaÃ§Ãµes de data continham apenas data, sem hora. Por isso, o DATE Ã© o melhor por armazenar apenas o que queremos que Ã© o que Ã© entregue pelos registros.

#### 3 -> 3.3. Elabore queries para importar o conteÃºdo dos arquivos CSV:

- Realizando a anÃ¡lise crÃ­tica durante a importaÃ§Ã£o dos dados dos arquivos csv:
  - **Valores NULL em campos obrigatÃ³rios:** Implementei um filtro (dropna) para descartar registros de operadoras que nÃ£o possuÃ­ssem CNPJ ou RazÃ£o Social, garantindo a integridade da constraint NOT NULL do banco de dados.
  - **Strings em campos numÃ©ricos:** Utilizei o parÃ¢metro decimal="," na leitura do Pandas para interpretar corretamente o formato brasileiro de moeda, convertendo strings numÃ©ricas para float antes da inserÃ§Ã£o.
  - **Datas em formatos inconsistentes:** Apliquei pd.to_datetime(..., errors='coerce'). Datas invÃ¡lidas foram convertidas para NULL (permitido na modelagem), preservando o registro em vez de descartÃ¡-lo, mas mantendo a consistÃªncia do tipo de dado DATE.

#### 3 -> 3.4. Desenvolva queries analÃ­ticas para responder:

- Explicando o trade-off da Query 1: Para o cÃ¡lculo de crescimento, utilizei um INNER JOIN entre os dados do primeiro e do Ãºltimo trimestre. Isso filtra automaticamente operadoras que nÃ£o possuem dados em um dos dois perÃ­odos. Essa abordagem foi escolhida porque, matematicamente, nÃ£o Ã© possÃ­vel calcular a taxa de crescimento se o valor inicial for inexistente (nulo) ou zero (o que causaria erro de divisÃ£o por zero). Portanto, apenas operadoras com atividade contÃ­nua no perÃ­odo analisado foram consideradas.

- Explicando o trade-off da Query 3: Optei pela abordagem utilizando CTEs (Common Table Expressions) (WITH media_global AS...) em vez de Subqueries aninhadas no WHERE ou JOINs complexos. Pois, melhora a legibilidade do cÃ³digo, tendo um passo-Ã -passo de como cada linha se comporta atÃ© o resultado final. Melhora a manutenibilidade do cÃ³digo, visto que evita alterar toda a query sql. Basta apenas alterar alguma CTE conforme o que for pedido.

### Item 4: TESTE DE API E INTERFACE WEB

#### 4 -> 4.2.1. Escolha do Framework:

- Para o Framework utilizado na criaÃ§Ã£o da API, foi escolhido a OpÃ§Ã£o B: **FastAPI**. Devido sua alta performance, visto que tem um padrÃ£o assÃ­ncrono jÃ¡ nativo, utiliza tipagem nativa o que torna mais fÃ¡cil e seguro a validaÃ§Ã£o de dados e possui geraÃ§Ã£o automÃ¡tica de documentaÃ§Ã£o (Swagger UI), agilizando os teste com as rotas da API, tornando a manutenÃ§Ã£o mais rÃ¡pida e tirando a necessidade do uso do Postman.

#### 4 -> 4.2.2. EstratÃ©gia de PaginaÃ§Ã£o:

- EstratÃ©gia de PaginaÃ§Ã£o Escolhida: OpÃ§Ã£o A: **Offset-based**. Ã‰ o mÃ©todo padrÃ£o utilizado na maioria das APIs REST, por lidar com (LIMIT/OFFSET), facilitando integraÃ§Ã£o com o frontend que apenas recebe os parÃ¢metros page e limit e facilita a UX. AlÃ©m disso, como nÃ£o hÃ¡ uma quantidade brutal de dados, nÃ£o se faz necessÃ¡rio o uso de alguma abordagem mais complexa como o Cursor-based. Portanto, o Offset-based Ã© o mais simples e intuitivo diariamente e para um projeto assim com quantidade de dados razoÃ¡veis, mas nÃ£o extravagantes demais.

#### 4 -> 4.2.3. Cache vs Queries Diretas:

- EstratÃ©gia Escolhida: OpÃ§Ã£o A: **Calcular sempre na hora**. Pois, dado o tamanho do projeto e o volume de dados ser controlado (nÃ£o ser tÃ£o exorbitante), optei por calcular sempre na hora utilizando queries de agregaÃ§Ã£o (SUM, AVG) diretamente na query SQL no Banco de dados. Assim, possibilitando simplicidade, visto que reduz a complexidade da arquitetura sem introduzir componentes extras como Redis ou tabelas temporÃ¡rias e garantindo que o dado exibido Ã© sempre o dado real do momento, sem risco de cache antigo.

##### ObservaÃ§Ã£o: Em um cenÃ¡rio de produÃ§Ã£o com milhÃµes de acessos, eu migraria para a OpÃ§Ã£o B (Cachear resultado por X minutos), utilizando o Redis com um tempo de 10 a 60 minutos.

#### 4 -> 4.2.4. Estrutura de Resposta da API:

- Estrutura de Resposta da API: OpÃ§Ã£o B: **Dados + Metadados**. O uso de dados + metadados Ã© mais interessante para projetos que contenham um Dashboard, como este. Pois, eles possuem registros com alto nÃºmero de dados, o que impossibilita o retorno somente dos dados, pois sem controle de pÃ¡ginas e com muitos dados, o Frontend teria uma tabela com scroll infinito, o que quebra a UX. Portanto, ao retornar os metadados o Frontend pode ter controle das pÃ¡ginas e retornar somente o nÃºmero correto de dados por pÃ¡gina (baseado no LIMIT/OFFSET). Assim, permite uma UI melhor de paginaÃ§Ã£o, mostrar a pÃ¡gina que o usuÃ¡rio estÃ¡ dentre a quantidade de pÃ¡ginas existentes, assim melhorando a UX.

#### 4 -> 4.3.1. EstratÃ©gia de Busca/Filtro:

- EstratÃ©gia de Busca/Filtro: OpÃ§Ã£o A: **Busca no Servidor**. Pois,
  - **Volume de Dados e Escalabilidade:** O cadastro de operadoras da ANS e seus dados histÃ³ricos representam um conjunto de dados potencialmente grande e em crescimento contÃ­nuo. Carregar todos os registros para o navegador do cliente (Client-side) de uma Ãºnica vez resultaria em um payload inicial muito pesado, aumentando drasticamente o tempo de carregamento da aplicaÃ§Ã£o e consumindo memÃ³ria excessiva do dispositivo do usuÃ¡rio. A busca no servidor garante que o frontend receba apenas o subconjunto de dados preciso, mantendo a aplicaÃ§Ã£o leve e performÃ¡tica independente do tamanho total do banco de dados.
  - **ExperiÃªncia do UsuÃ¡rio (UX):** Embora a busca no servidor introduza uma latÃªncia de rede a cada requisiÃ§Ã£o, ela evita o travamento da interface que ocorreria ao tentar filtrar arrays com milhares de objetos via JavaScript no navegador. Utilizando eventos como @change, garantimos que o usuÃ¡rio tenha uma resposta precisa e atualizada diretamente da fonte da verdade (banco de dados), sem riscos de dados obsoletos em cache local.

#### 4 -> 4.3.2. Gerenciamento de Estado:

- Para gerenciar os dados das operadoras: OpÃ§Ã£o C: **Composables (Vue 3)**. Pois,
  - **Complexidade da AplicaÃ§Ã£o:** O escopo atual da aplicaÃ§Ã£o foca em um dashboard onde os dados sÃ£o consumidos e exibidos no mesmo componente, componente especÃ­fico para mostrar os dados, logo, nÃ£o hÃ¡ necessidade de compartilhar a lista de operadoras com outros componentes distantes na Ã¡rvore da aplicaÃ§Ã£o (como um rodapÃ©, sidebar ou carrinho de compras), o que torna o uso de uma biblioteca de gerenciamento global (como Pinia ou Vuex) uma complexidade desnecessÃ¡ria.
  - **Manutenibilidade e Simplicidade:** A simplicidade que o uso do Reactivity API (ref) junto com os componentes de renderizaÃ§Ã£o (onMounted, onUpdated) Ã© um estratÃ©gia simples, porÃ©m muito forte. O ref consegue gerenciar estados perfeitamente, e, caso a aplicaÃ§Ã£o cresÃ§a no futuro, a lÃ³gica de refs pode ser extraÃ­da para Composables reutilizÃ¡veis, tornando essa ideia ainda mais forte.

#### 4 -> 4.3.3. Performance da Tabela:

- Para exibir muitas operadoras, a estratÃ©gia utilizada foi a renderizaÃ§Ã£o padrÃ£o com paginaÃ§Ã£o no servidor. Pois,
  - **Volume de Dados:** Embora o banco de dados possa conter milhares de registros, a aplicaÃ§Ã£o nunca renderiza todos de uma vez. Ao limitar a resposta a 10 itens por pÃ¡gina (devido ao LIMIT no backend), o nÃºmero de nÃ³s no DOM (Document Object Model) permanece constante e baixo. Isso torna desnecessÃ¡rio o uso de tÃ©cnicas complexas como virtualizaÃ§Ã£o, que sÃ³ se justificariam se precisÃ¡ssemos exibir centenas de linhas simultaneamente na mesma tela.
  - **Requisitos de UX:** A paginaÃ§Ã£o tradicional oferece uma navegaÃ§Ã£o previsÃ­vel e permite que o usuÃ¡rio tenha uma noÃ§Ã£o clara de "onde estÃ¡" (Ex: PÃ¡gina 2 de 50). A renderizaÃ§Ã£o padrÃ£o do Vue.js Ã© extremamente eficiente para listas pequenas, garantindo transiÃ§Ãµes de pÃ¡gina instantÃ¢neas e baixo consumo de memÃ³ria no navegador do cliente, mesmo em dispositivos mÃ³veis.

#### 4 -> 4.3.4. Tratamento de Erros e Loading:

- Como eu trato erros e loading:
  - **Erros de Rede/API:** Implementado atravÃ©s de blocos try/catch na funÃ§Ã£o assÃ­ncrona, como na funÃ§Ã£o loadOperators. Em caso de falha, o erro Ã© capturado e logado no console para depuraÃ§Ã£o (console.error). Para evitar que a interface dÃª um crash, o estado Ã© resetado para uma lista vazia, garantindo que a aplicaÃ§Ã£o continue funcional mesmo sem dados.
  - **Estados de Loading:** Ã‰ utilizado renderizaÃ§Ã£o condiciona do Vue.js (v-if/v-else). Enquanto a variÃ¡vel que armazena os dados Ã© null, um spinner centralizado Ã© exibido. Isso previne uma mudanÃ§a brusca de layout, quebrando a UX e informa visualmente ao usuÃ¡rio que o sistema estÃ¡ processando a requisiÃ§Ã£o. Assim, a interface de dados sÃ³ Ã© montada apÃ³s a resposta completa da API.
  - **Dados Vazios (Empty State):** VerificaÃ§Ã£o explÃ­cita do tamanho do array (ex: operatorsData.operators.length === 0). Caso a busca retorne sucesso (200 OK) mas sem resultados, exibe-se uma mensagem amigÃ¡vel ("Nenhuma operadora encontrada...") em vez de uma tabela vazia com cabeÃ§alhos Ã³rfÃ£os, melhorando a clareza para o usuÃ¡rio. Esse comportamento evita o uso do status 404 no backend e o eventual crash no frontend.

- AnÃ¡lise crÃ­tica (mensagens genÃ©ricas ou especÃ­ficas): Utilizei uma abordagem hÃ­brida (feedback especÃ­fico para contexto, mas genÃ©rico para erros de sistema). Quando o frontend realiza uma requisiÃ§Ã£o e nÃ£o retorna dados ou dÃ¡ algum erro, mostramos o que aconteceu, uma mensagem especÃ­fica, como Ã© o caso da mensagem "Nenhuma operadora encontrada...". Para erros de sistema, evitamos mostrar os erros reais, pois o usuÃ¡rio pode nÃ£o entender algum erro dentro do servidor, por exemplo, e isso pode ser um problema. Por isso apenas mostra uma mensagem genÃ©rica de erro.

---

## ğŸŒŸ Diferenciais Implementados

- **ğŸ¤– AutomaÃ§Ã£o Completa:** Scripts de download e ETL resilientes que eliminam intervenÃ§Ã£o manual.
- **âš¡ Performance:** Uso de streaming para downloads e leituras otimizadas com Pandas.
- **ğŸ“ ORM - Banco de dados:** Uso do ORM SQLAlchemy para facilitar a criaÃ§Ã£o das tabelas no banco de dados, tornando o processo mais eficiente e aumentando a performance, devido a integraÃ§Ã£o do SQLAlchemy com o Pandas.
- **ğŸ“Š Nova rota "/api/operadoras/{cnpj}/despesas/chart":** Rota que leva os dados para popular o grÃ¡fico na pÃ¡gina de detalhes com as despesas de cada trimestre da operadora.
- **ğŸ“Š VisualizaÃ§Ã£o Rica:** Frontend interativo com grÃ¡ficos (Chart.js) e tratamento de erros de UX.
- **ğŸ“Š VisualizaÃ§Ã£o top 5 maiores despesas:** SeÃ§Ã£o com o ranking das 5 operadoras com mais despesas.
- **ğŸ“ DocumentaÃ§Ã£o Viva:** Uso do Swagger UI para documentaÃ§Ã£o interativa da API.
- **ğŸ—‚ï¸ Versionamento:** HistÃ³rico Git estruturado.

---

AgradeÃ§o a oportunidade de participar desse projeto. Foi uma experiÃªncia incrÃ­vel.

- **Autor:** Gustavo Luiz Scobernatti de Almeida.
- **Links importantes para contato:**
  - **Github:** https://github.com/GuScobernatti
  - **LinkedIn:** https://www.linkedin.com/in/gustavo-scobernatti/
  - **WhatsApp:** 33984630077
